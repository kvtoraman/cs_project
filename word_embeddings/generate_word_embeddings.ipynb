{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.utils import pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC,LinearSVC,SVR\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error,roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"\" + REVIEWS + \".txt\",\"r\",encoding=\"utf-8\").readlines()\n",
    "# #print each sentence in a line\n",
    "# with open(\"\" + REVIEWS+\"_whole_sentence.txt\",\"w\",encoding=\"utf-8\") as out:\n",
    "#     for line in f:\n",
    "#         course = json.loads(line)\n",
    "#         review_text = course[\"review_text\"].strip()\n",
    "#         out.write(review_text + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from konlpy.tag import Okt\n",
    "import argparse\n",
    "import re\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Map(dict):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    m = Map({'first_name': 'Eduardo'}, last_name='Pool', age=24, sports=['Soccer'])\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Map, self).__init__(*args, **kwargs)\n",
    "        for arg in args:\n",
    "            if isinstance(arg, dict):\n",
    "                for k, v in arg.iteritems():\n",
    "                    self[k] = v\n",
    "\n",
    "        if kwargs:\n",
    "            for k, v in kwargs.iteritems():\n",
    "                self[k] = v\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return self.get(attr)\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self.__setitem__(key, value)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        super(Map, self).__setitem__(key, value)\n",
    "        self.__dict__.update({key: value})\n",
    "\n",
    "    def __delattr__(self, item):\n",
    "        self.__delitem__(item)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        super(Map, self).__delitem__(key)\n",
    "        del self.__dict__[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Step 1 : Parse Arguments.\n",
    "'''\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"input\", type=str, help=\"input text file for training: one sentence per line\")\n",
    "# parser.add_argument(\"--embedding_size\", type=int, help=\"embedding vector size (default=150)\", default=150)\n",
    "# parser.add_argument(\"--window_size\", type=int, help=\"window size (default=5)\", default=5)\n",
    "# parser.add_argument(\"--min_count\", type=int, help=\"minimal number of word occurences (default=5)\", default=5)\n",
    "# parser.add_argument(\"--num_sampled\", type=int, help=\"number of negatives sampled (default=50)\", default=50)\n",
    "# parser.add_argument(\"--learning_rate\", type=float, help=\"learning rate (default=1.0)\", default=1.0)\n",
    "# parser.add_argument(\"--sampling_rate\", type=int, help=\"rate for subsampling frequent words (default=0.0001)\", default=0.0001)\n",
    "# parser.add_argument(\"--epochs\", type=int, help=\"number of epochs (default=3)\", default=3)\n",
    "# parser.add_argument(\"--batch_size\", type=int, help=\"batch size (default=150)\", default=150)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "REVIEWS = \"all_reviews\"\n",
    "POS_TYPE = \"whole_sentence\"\n",
    "# POS_TYPE = \"nouns\"\n",
    "\n",
    "args = Map()\n",
    "args.input = REVIEWS + \"_\" + POS_TYPE +\".txt\"\n",
    "args.embedding_size = 50\n",
    "args.window_size = 5\n",
    "args.min_count = 3\n",
    "args.num_sampled = 50\n",
    "args.learning_rate = 0.9\n",
    "args.sampling_rate = 0.0001\n",
    "args.epochs = 3\n",
    "args.batch_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Step 2 : Pre-process Data.\n",
    "'''\n",
    "\n",
    "def build_dataset(train_text, min_count, sampling_rate):\n",
    "    words = list()\n",
    "    with open(train_text, 'r',encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            sentence = re.sub(r\"[^ㄱ-힣a-zA-Z0-9]+\", ' ', line).strip().split()\n",
    "            if sentence:\n",
    "                words.append(sentence)\n",
    "\n",
    "    word_counter = [['UNK', -1]]\n",
    "    word_counter.extend(collections.Counter([word for sentence in words for word in sentence]).most_common())\n",
    "    word_counter = [item for item in word_counter if item[1] >= min_count or item[0] == 'UNK']\n",
    "\n",
    "    word_dict = dict()\n",
    "    for word, count in word_counter:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    word_reverse_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    word_to_pos_li = dict()\n",
    "    pos_list = list()\n",
    "    okt = Okt()\n",
    "    for w in word_dict:\n",
    "        w_pos_li = list()\n",
    "        for pos in okt.pos(w, norm=True):\n",
    "            w_pos_li.append(pos)\n",
    "\n",
    "        word_to_pos_li[word_dict[w]] = w_pos_li\n",
    "        pos_list += w_pos_li\n",
    "\n",
    "    pos_counter = collections.Counter(pos_list).most_common()\n",
    "\n",
    "    pos_dict = dict()\n",
    "    for pos, _ in pos_counter:\n",
    "        pos_dict[pos] = len(pos_dict)\n",
    "\n",
    "    pos_reverse_dict = dict(zip(pos_dict.values(), pos_dict.keys()))\n",
    "\n",
    "    word_to_pos_dict = dict()\n",
    "\n",
    "    for word_id, pos_li in word_to_pos_li.items():\n",
    "        pos_id_li = list()\n",
    "        for pos in pos_li:\n",
    "            pos_id_li.append(pos_dict[pos])\n",
    "        word_to_pos_dict[word_id] = pos_id_li\n",
    "\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for sentence in words:\n",
    "        s = list()\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                index = word_dict[word]\n",
    "            else:\n",
    "                index = word_dict['UNK']\n",
    "                unk_count += 1\n",
    "            s.append(index)\n",
    "        data.append(s)\n",
    "    word_counter[0][1] = max(1, unk_count)\n",
    "\n",
    "    data = sub_sampling(data, word_counter, word_dict, sampling_rate)\n",
    "\n",
    "    return data, word_dict, word_reverse_dict, pos_dict, pos_reverse_dict, word_to_pos_dict\n",
    "\n",
    "\n",
    "# Sub-sampling frequent words according to sampling_rate\n",
    "def sub_sampling(data, word_counter, word_dict, sampling_rate):\n",
    "    total_words = sum([len(sentence) for sentence in data])\n",
    "    prob_dict = dict()\n",
    "    for word, count in word_counter:\n",
    "        f = count / total_words\n",
    "        p = max(0, 1 - math.sqrt(sampling_rate / f))\n",
    "        prob_dict[word_dict[word]] = p\n",
    "\n",
    "    new_data = list()\n",
    "    for sentence in data:\n",
    "        s = list()\n",
    "        for word in sentence:\n",
    "            prob = prob_dict[word]\n",
    "            if random.random() > prob:\n",
    "                s.append(word)\n",
    "        new_data.append(s)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences : 5396\n",
      "vocabulary size : 15000\n",
      "pos size : 8300\n"
     ]
    }
   ],
   "source": [
    "data, word_dict, word_reverse_dict, pos_dict, pos_reverse_dict, word_to_pos_dict \\\n",
    "        = build_dataset(args.input, args.min_count, args.sampling_rate)\n",
    "\n",
    "vocabulary_size = len(word_dict)\n",
    "pos_size = len(pos_dict)\n",
    "num_sentences = len(data)\n",
    "\n",
    "print(\"number of sentences :\", num_sentences)\n",
    "print(\"vocabulary size :\", vocabulary_size)\n",
    "print(\"pos size :\", pos_size)\n",
    "\n",
    "pos_li = []\n",
    "for key in sorted(pos_reverse_dict):\n",
    "    pos_li.append(pos_reverse_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Step 3 : Function to generate a training batch\n",
    "'''\n",
    "\n",
    "window_size = args.window_size\n",
    "batch_size = args.batch_size\n",
    "\n",
    "\n",
    "def generate_input_output_list(data, window_size):\n",
    "    input_li = list()\n",
    "    output_li = list()\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)):\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    if sentence[i]!=word_dict['UNK'] and sentence[j]!=word_dict['UNK']:\n",
    "                        input_li.append(sentence[i])\n",
    "                        output_li.append(sentence[j])\n",
    "    return input_li, output_li\n",
    "\n",
    "input_li, output_li = generate_input_output_list(data, window_size)\n",
    "input_li_size = len(input_li)\n",
    "\n",
    "\n",
    "def generate_batch(iter, batch_size):\n",
    "    # print(input_li_size)\n",
    "    # print(batch_size)\n",
    "    index = (iter % (input_li_size//batch_size)) * batch_size\n",
    "    batch_input = input_li[index:index+batch_size]\n",
    "    batch_output_li = output_li[index:index+batch_size]\n",
    "    batch_output = [[i] for i in batch_output_li]\n",
    "\n",
    "    return np.array(batch_input), np.array(batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Step 4 : Build a model.\n",
    "'''\n",
    "\n",
    "embedding_size = args.embedding_size\n",
    "num_sampled = args.num_sampled\n",
    "learning_rate = args.learning_rate\n",
    "\n",
    "valid_size = 20     # Random set of words to evaluate similarity on.\n",
    "valid_window = 200  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    words_matrix = [tf.placeholder(tf.int32, shape=None) for _ in range(batch_size)]\n",
    "    vocabulary_matrix = [tf.placeholder(tf.int32, shape=(None)) for _ in range(vocabulary_size)]\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        pos_embeddings = tf.Variable(tf.random_uniform([pos_size, embedding_size], -1.0, 1.0), name='pos_embeddings')\n",
    "\n",
    "        word_vec_list = []\n",
    "        for i in range(batch_size):\n",
    "            word_vec = tf.reduce_sum(tf.nn.embedding_lookup(pos_embeddings, words_matrix[i]), 0)\n",
    "            word_vec_list.append(word_vec)\n",
    "        word_embeddings = tf.stack(word_vec_list)\n",
    "\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)), name='nce_weights'\n",
    "        )\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]), name='nce_biases')\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_labels,\n",
    "                       inputs=word_embeddings,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Compute the cosine similarity between minibatch exaples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(pos_embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = pos_embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "\n",
    "# Function to save vectors.\n",
    "def save_model(pos_list, embeddings, file_name):\n",
    "    with open(file_name, 'w',encoding=\"utf-8\") as f:\n",
    "        f.write(str(len(pos_list)))\n",
    "        f.write(\" \")\n",
    "        f.write(str(embedding_size))\n",
    "        f.write(\"\\n\")\n",
    "        for i in range(len(pos_list)):\n",
    "            pos = pos_list[i]\n",
    "            f.write(str(pos).replace(\"', '\", \"','\") + \" \")\n",
    "            f.write(' '.join(map(str, embeddings[i])))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iterations for each epoch : 11735\n",
      "Initialized - Tensorflow\n",
      "Average loss at step  0 :  189.6513671875\n",
      "Nearest to ('부담', 'Noun'): ('웃도는', 'Verb'), ('프리', 'Noun'), ('하면서', 'Verb'), ('하는지', 'Verb'), ('즐길', 'Verb'), ('만', 'Noun'), ('알았어요', 'Verb'), ('부를', 'Verb'),\n",
      "Nearest to ('게', 'Josa'): ('저희', 'Noun'), ('궁금하네요', 'Adjective'), ('reaction', 'Alpha'), ('point', 'Alpha'), ('서', 'Josa'), ('섞어서', 'Verb'), ('손해', 'Noun'), ('Systems', 'Alpha'),\n",
      "Nearest to ('보다는', 'Josa'): ('잡은', 'Verb'), ('나왔는지', 'Verb'), ('되어야', 'Verb'), ('EE', 'Alpha'), ('건가', 'Noun'), ('듣거나', 'Verb'), ('don', 'Alpha'), ('만들어도', 'Verb'),\n",
      "Nearest to ('들어', 'Verb'): ('괴랄', 'Noun'), ('마냥', 'Noun'), ('아는게', 'Verb'), ('Pre', 'Alpha'), ('갑', 'Noun'), ('치고', 'Josa'), ('함수', 'Noun'), ('않습니다', 'Verb'),\n",
      "Nearest to ('4', 'Number'): ('주어집니다', 'Verb'), ('합', 'Noun'), ('판서', 'Noun'), ('짜', 'Verb'), ('끝난', 'Verb'), ('보면', 'Verb'), ('얻을', 'Verb'), ('그랬던', 'Adjective'),\n",
      "Nearest to ('지', 'Josa'): ('point', 'Alpha'), ('나긋나긋', 'Adverb'), ('따기', 'Verb'), ('100', 'Number'), ('컨트롤', 'Noun'), ('tough', 'Alpha'), ('피해', 'Noun'), ('학', 'Noun'),\n",
      "Nearest to ('추천', 'Noun'): ('보면서도', 'Verb'), ('라고', 'Josa'), ('작지', 'Adjective'), ('있으신', 'Adjective'), ('6시', 'Number'), ('hours', 'Alpha'), ('같긴', 'Adjective'), ('모든', 'Noun'),\n",
      "Nearest to ('들', 'Suffix'): ('클래식', 'Noun'), ('EE', 'Alpha'), ('쉬웠던', 'Adjective'), ('아시다시피', 'Verb'), ('그랬습니다', 'Adjective'), ('없다는', 'Adjective'), ('안된다', 'Adjective'), ('같음', 'Adjective'),\n",
      "Nearest to ('번', 'Noun'): ('개꿀띠', 'Noun'), ('받아들이고', 'Verb'), ('유전', 'Noun'), ('넓은', 'Adjective'), ('독일', 'Noun'), ('ML', 'Alpha'), ('크고', 'Verb'), ('좋아하시는데', 'Adjective'),\n",
      "Nearest to ('공부', 'Noun'): ('그리피스', 'Noun'), ('N', 'Alpha'), ('다르게', 'Adjective'), ('논리', 'Noun'), ('배우며', 'Verb'), ('알고리즘', 'Noun'), ('머릿속', 'Noun'), ('가르치', 'Verb'),\n",
      "Nearest to ('A', 'Alpha'): ('틀려서', 'Verb'), ('떴습니다', 'Verb'), ('이산수학', 'Noun'), ('막판', 'Noun'), ('유쾌하고', 'Adjective'), ('주어졌습니다', 'Verb'), ('놓친', 'Verb'), ('바로', 'Noun'),\n",
      "Nearest to ('을', 'Josa'): ('돈', 'Noun'), ('핸드폰', 'Noun'), ('나서', 'Verb'), ('22', 'Number'), ('치명', 'Noun'), ('허나', 'Verb'), ('방안', 'Noun'), ('정성', 'Noun'),\n",
      "Nearest to ('만점', 'Noun'): ('냈습니다', 'Verb'), ('간격', 'Noun'), ('봐주십니다', 'Verb'), ('말씀드리면', 'Verb'), ('주시네요', 'Verb'), ('세줄', 'Verb'), ('개강', 'Noun'), ('가', 'Verb'),\n",
      "Nearest to ('발표', 'Noun'): ('어땠고', 'Adjective'), ('확률', 'Noun'), ('피곤한', 'Adjective'), ('받아주시고', 'Verb'), ('까다로웠습니다', 'Adjective'), ('놓은', 'Verb'), ('걸리는', 'Verb'), ('they', 'Alpha'),\n",
      "Nearest to ('집중', 'Noun'): ('막상', 'Noun'), ('In', 'Alpha'), ('꽉', 'Noun'), ('받았던', 'Verb'), ('Hour', 'Alpha'), ('필요하다면', 'Adjective'), ('힘든', 'Adjective'), ('요인', 'Noun'),\n",
      "Nearest to ('명', 'Noun'): ('싶은게', 'Verb'), ('만으로', 'Josa'), ('될것', 'Verb'), ('됐는데', 'Verb'), ('유용하게', 'Adjective'), ('know', 'Alpha'), ('스포츠', 'Noun'), ('드립', 'Noun'),\n",
      "Nearest to ('답', 'Noun'): ('Final', 'Alpha'), ('보내는', 'Verb'), ('프리', 'Noun'), ('했다는', 'Verb'), ('기', 'Noun'), ('틀렸다', 'Verb'), ('많은게', 'Adjective'), ('찾다가', 'Verb'),\n",
      "Nearest to ('해', 'Verb'): ('quite', 'Alpha'), ('놀라운', 'Adjective'), ('하진', 'Noun'), ('지적', 'Noun'), ('스', 'Noun'), ('나더군요', 'Verb'), ('볼츠만', 'Noun'), ('점점', 'Noun'),\n",
      "Nearest to ('작성', 'Noun'): ('다가올', 'Verb'), ('보여주기도', 'Verb'), ('틀리게', 'Verb'), ('소질', 'Noun'), ('천', 'Modifier'), ('presentation', 'Alpha'), ('자신있게', 'Verb'), ('알아볼', 'Verb'),\n",
      "Nearest to ('학점', 'Noun'): ('들을게', 'Verb'), ('포함', 'Noun'), ('억지로', 'Noun'), ('엔', 'Josa'), ('1시', 'Number'), ('정확한', 'Adjective'), ('크지만', 'Verb'), ('그러면', 'Adjective'),\n",
      "Average loss at step  2000 :  56.84043586063385\n",
      "Average loss at step  4000 :  17.74194978237152\n",
      "Average loss at step  6000 :  10.17324472284317\n",
      "Average loss at step  8000 :  7.838280810832977\n",
      "Average loss at step  10000 :  6.614841792583466\n",
      "Average loss at step  12000 :  6.132587151765823\n",
      "Average loss at step  14000 :  5.829978402376175\n",
      "Average loss at step  16000 :  5.738689568042755\n",
      "Average loss at step  18000 :  5.624516903400421\n",
      "Average loss at step  20000 :  5.601738988876343\n",
      "Nearest to ('부담', 'Noun'): ('프리', 'Noun'), ('thm', 'Alpha'), ('즐길', 'Verb'), ('정도', 'Noun'), ('담론', 'Noun'), ('connectedness', 'Alpha'), ('배우게', 'Verb'), ('러닝', 'Noun'),\n",
      "Nearest to ('게', 'Josa'): ('서', 'Josa'), ('쓰거나', 'Verb'), ('아니었지만', 'Adjective'), ('지는', 'Josa'), ('reaction', 'Alpha'), ('궁금하네요', 'Adjective'), ('Systems', 'Alpha'), ('2017', 'Number'),\n",
      "Nearest to ('보다는', 'Josa'): ('잡은', 'Verb'), ('강력하게', 'Adjective'), ('나왔는지', 'Verb'), ('배운것을', 'Verb'), ('논어', 'Noun'), ('건가', 'Noun'), ('가기', 'Noun'), ('조교', 'Noun'),\n",
      "Nearest to ('들어', 'Verb'): ('괴랄', 'Noun'), ('졸릴', 'Verb'), ('하고', 'Verb'), ('마냥', 'Noun'), ('Search', 'Alpha'), ('분', 'Noun'), ('아니다', 'Adjective'), ('현장', 'Noun'),\n",
      "Nearest to ('4', 'Number'): ('Standard', 'Alpha'), ('2', 'Number'), ('이런', 'Modifier'), ('퀴즈', 'Noun'), ('합', 'Noun'), ('정도', 'Noun'), ('중간고사', 'Noun'), ('과제', 'Noun'),\n",
      "Nearest to ('지', 'Josa'): ('따기', 'Verb'), ('컨트롤', 'Noun'), ('학', 'Noun'), ('point', 'Alpha'), ('짧습니다', 'Adjective'), ('피해', 'Noun'), ('안된', 'Adjective'), ('있을까', 'Adjective'),\n",
      "Nearest to ('추천', 'Noun'): ('교수', 'Noun'), ('라고', 'Josa'), ('ligase', 'Alpha'), ('것', 'Noun'), ('보면서도', 'Verb'), ('내용', 'Noun'), ('compactness', 'Alpha'), ('겁니다', 'Verb'),\n",
      "Nearest to ('들', 'Suffix'): ('기생', 'Noun'), ('서도', 'Noun'), ('대수학', 'Noun'), ('에서만', 'Josa'), ('이면', 'Foreign'), ('그만한', 'Adjective'), ('그러셨습니다', 'Adjective'), ('드랍하', 'Noun'),\n",
      "Nearest to ('번', 'Noun'): ('과제', 'Noun'), ('정도', 'Noun'), ('중간', 'Noun'), ('합니다', 'Verb'), ('점수', 'Noun'), ('퀴즈', 'Noun'), ('해서', 'Verb'), ('시험', 'Noun'),\n",
      "Nearest to ('공부', 'Noun'): ('합니다', 'Verb'), ('이런', 'Modifier'), ('내용', 'Noun'), ('connectedness', 'Alpha'), ('과목', 'Noun'), ('ligase', 'Alpha'), ('시험', 'Noun'), ('과제', 'Noun'),\n",
      "Nearest to ('A', 'Alpha'): ('성적', 'Noun'), ('중간', 'Noun'), ('connectedness', 'Alpha'), ('평균', 'Noun'), ('권길헌', 'Noun'), ('시험', 'Noun'), ('잘', 'Verb'), ('던지지', 'Verb'),\n",
      "Nearest to ('을', 'Josa'): ('감상', 'Noun'), ('d', 'Alpha'), ('나오는거', 'Verb'), ('있는가를', 'Adjective'), ('흥미로웠어요', 'Adjective'), ('Comic', 'Alpha'), ('따라가기는', 'Verb'), ('스포츠', 'Noun'),\n",
      "Nearest to ('만점', 'Noun'): ('번', 'Noun'), ('성적', 'Noun'), ('과제', 'Noun'), ('점수', 'Noun'), ('중간', 'Noun'), ('시험', 'Noun'), ('정도', 'Noun'), ('듣는것을', 'Verb'),\n",
      "Nearest to ('발표', 'Noun'): ('connectedness', 'Alpha'), ('이런', 'Modifier'), ('과제', 'Noun'), ('내용', 'Noun'), ('것', 'Noun'), ('Standard', 'Alpha'), ('정도', 'Noun'), ('해서', 'Verb'),\n",
      "Nearest to ('집중', 'Noun'): ('Search', 'Alpha'), ('강의', 'Noun'), ('꽉', 'Noun'), ('내용', 'Noun'), ('Part', 'Alpha'), ('생각', 'Noun'), ('준', 'Noun'), ('공부', 'Noun'),\n",
      "Nearest to ('명', 'Noun'): ('know', 'Alpha'), ('도움', 'Noun'), ('합니다', 'Verb'), ('준비', 'Noun'), ('Standard', 'Alpha'), ('이후', 'Noun'), ('새', 'Noun'), ('됐는데', 'Verb'),\n",
      "Nearest to ('답', 'Noun'): ('2', 'Number'), ('정도', 'Noun'), ('안', 'Noun'), ('프리', 'Noun'), ('Final', 'Alpha'), ('만들어도', 'Verb'), ('숙제', 'Noun'), ('보내는', 'Verb'),\n",
      "Nearest to ('해', 'Verb'): ('하는거', 'Verb'), ('똑똑하신', 'Adjective'), ('중요하긴', 'Adjective'), ('깔끔하게', 'Adjective'), ('비슷할', 'Adjective'), ('낮았습니다', 'Adjective'), ('부족하다고', 'Adjective'), ('되었다', 'Verb'),\n",
      "Nearest to ('작성', 'Noun'): ('소질', 'Noun'), ('자신있게', 'Verb'), ('보여주기도', 'Verb'), ('다가올', 'Verb'), ('관해', 'Noun'), ('presentation', 'Alpha'), ('주어진', 'Verb'), ('다시', 'Noun'),\n",
      "Nearest to ('학점', 'Noun'): ('성적', 'Noun'), ('과목', 'Noun'), ('Search', 'Alpha'), ('connectedness', 'Alpha'), ('사람', 'Noun'), ('중간', 'Noun'), ('생각', 'Noun'), ('바둑', 'Noun'),\n",
      "Average loss at step  22000 :  5.570422158002853\n",
      "Average loss at step  24000 :  5.576063861489296\n",
      "Average loss at step  26000 :  5.554105318069458\n",
      "Average loss at step  28000 :  5.513931137561798\n",
      "Average loss at step  30000 :  5.4778532955646515\n",
      "Average loss at step  32000 :  5.465274864077568\n",
      "Average loss at step  34000 :  5.416238449215889\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Step 5 : Train a model.\n",
    "'''\n",
    "\n",
    "num_iterations = input_li_size // batch_size\n",
    "print(\"number of iterations for each epoch :\", num_iterations)\n",
    "epochs = args.epochs\n",
    "num_steps = num_iterations * epochs + 1\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print(\"Initialized - Tensorflow\")\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(step, batch_size)\n",
    "\n",
    "        word_list = []\n",
    "        for word in batch_inputs:\n",
    "            word_list.append(word_to_pos_dict[word])\n",
    "\n",
    "        feed_dict = {}\n",
    "        for i in range(batch_size):\n",
    "            feed_dict[words_matrix[i]] = word_list[i]\n",
    "        feed_dict[train_inputs] = batch_inputs\n",
    "        feed_dict[train_labels] = batch_labels\n",
    "\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        if step % 20000 == 0:\n",
    "            pos_embed = pos_embeddings.eval()\n",
    "\n",
    "            # Print nearest words\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_pos = pos_reverse_dict[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % str(valid_pos)\n",
    "                for k in range(top_k):\n",
    "                    close_word = pos_reverse_dict[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, str(close_word))\n",
    "                print(log_str)\n",
    "\n",
    "    # Save vectors\n",
    "    save_model(pos_li, pos_embeddings.eval(), \"we_\"+REVIEWS+\"_\"+POS_TYPE+\".vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp pos_nouns.vec ../../kor2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvtor\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Traceback (most recent call last):\n",
      "  File \"../../kor2vec/test/similarity_test.py\", line 55, in <module>\n",
      "    spearman, pearson, missed = word_sim_test(testset, pos_vectors)\n",
      "  File \"../../kor2vec/test/similarity_test.py\", line 28, in word_sim_test\n",
      "    with open(filename, 'r',encoding='utf-8') as pairs:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'test_dataset/kor_ws353.csv'\n"
     ]
    }
   ],
   "source": [
    "# !python ../../kor2vec/test/similarity_test.py pos_nouns.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
